{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc43585a",
   "metadata": {},
   "source": [
    "\n",
    "# Word2Vec on WikiTextâ€‘2: Endâ€‘toâ€‘End Walkthrough\n",
    "\n",
    "This notebook walks you stepâ€‘byâ€‘step through:\n",
    "1. **Loading the WikiTextâ€‘2 dataset** with [ðŸ¤— Datasets](https://huggingface.co/docs/datasets).\n",
    "2. **Preprocessing and tokenization** into sentences of tokens.\n",
    "3. **Creating Skipâ€‘Gram pairs** manually (for intuition and inspection).\n",
    "4. **Training Word2Vec (Skipâ€‘Gram)** with Gensim for distributional/meaning learning.\n",
    "5. **Quick sanityâ€‘checks & visualization** of learned semantics.\n",
    "\n",
    "> **Note:** You need an internet connection for the dataset download and `pip install` commands when you run this notebook locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbb688",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Setup\n",
    "\n",
    "Install the required packages (run this cell if you're in a fresh environment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63684537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on a clean environment, uncomment to install:\n",
    "# !pip install -U datasets gensim matplotlib numpy\n",
    "# (Optional) If you want to try a tiny PyTorch baseline later:\n",
    "# !pip install torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34646290",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load WikiTextâ€‘2\n",
    "We'll use the **raw** variant (`wikitext-2-raw-v1`) to handle our own tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadcf33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# This will download on first run and cache afterward\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Inspect splits and a sample\n",
    "print(ds)\n",
    "print(\"\\nExample train text snippet:\")\n",
    "print(ds[\"train\"][0][\"text\"][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c0c40",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Preprocess & Tokenize\n",
    "\n",
    "We'll do a **very light** tokenization:\n",
    "- Lowercase\n",
    "- Split on whitespace\n",
    "- Drop empty lines\n",
    "  \n",
    "For production, consider a robust tokenizer (e.g., spaCy, Hugging Face tokenizers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58451ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_sentences(dataset_split):\n",
    "    sentences = []\n",
    "    for ex in dataset_split:\n",
    "        text = ex[\"text\"].strip().lower()\n",
    "        if not text:\n",
    "            continue\n",
    "        # Very simple tokenization; you can replace with a better one\n",
    "        toks = text.split()\n",
    "        if toks:\n",
    "            sentences.append(toks)\n",
    "    return sentences\n",
    "\n",
    "train_sentences = to_sentences(ds[\"train\"])\n",
    "valid_sentences = to_sentences(ds[\"validation\"])\n",
    "test_sentences  = to_sentences(ds[\"test\"])\n",
    "\n",
    "len(train_sentences), len(valid_sentences), len(test_sentences), train_sentences[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d673e33",
   "metadata": {},
   "source": [
    "\n",
    "## 3. (Optional) Build Vocabulary\n",
    "\n",
    "If you want to **manually** create Skipâ€‘Gram pairs and analyze IDs, it's handy to build a vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count words on train sentences only\n",
    "counter = Counter(w for sent in train_sentences for w in sent)\n",
    "\n",
    "# Prune rare words to keep the vocab manageable (optional)\n",
    "min_count = 5\n",
    "vocab = [w for w, c in counter.items() if c >= min_count]\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "tok2id = {w: i for i, w in enumerate(vocab)}\n",
    "id2tok = {i: w for w, i in tok2id.items()}\n",
    "\n",
    "len(vocab), list(tok2id.items())[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a1d42",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Create Skipâ€‘Gram Pairs (Manually)\n",
    "\n",
    "We create `(target, context)` pairs from a window around each token.  \n",
    "This is for **intuition/inspection**â€”Gensim's Word2Vec can train Skipâ€‘Gram directly from sentences (`sg=1`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35910da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "WINDOW = 5  # context window on each side\n",
    "\n",
    "def skip_grams_from_sentences(sentences: List[List[str]], window: int = WINDOW):\n",
    "    pairs = []  # [[target_id, context_id], ...]\n",
    "    for sent in sentences:\n",
    "        for i, wd in enumerate(sent):\n",
    "            tid = tok2id.get(wd)\n",
    "            if tid is None:\n",
    "                continue\n",
    "            left = max(0, i - window)\n",
    "            right = min(len(sent), i + window + 1)\n",
    "            for j in range(left, right):\n",
    "                if j == i:\n",
    "                    continue\n",
    "                cid = tok2id.get(sent[j])\n",
    "                if cid is not None:\n",
    "                    pairs.append([int(tid), int(cid)])\n",
    "    return pairs\n",
    "\n",
    "# WARNING: Creating all pairs for the whole corpus can be large.\n",
    "# For demonstration, we'll take a small subset of sentences.\n",
    "subset = train_sentences[:2000]  # tweak as needed\n",
    "pairs_demo = skip_grams_from_sentences(subset, window=WINDOW)\n",
    "\n",
    "len(pairs_demo), pairs_demo[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e6373",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train Word2Vec (Skipâ€‘Gram) with Gensim\n",
    "\n",
    "Gensimâ€™s `Word2Vec` can train Skipâ€‘Gram internally by setting `sg=1`.  \n",
    "We feed it **sentences** (lists of tokens); it handles windowing and negative sampling under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Hyperparameters\n",
    "vector_size = 100\n",
    "window = 5           # context window size\n",
    "min_count = 5        # must match or be <= the pruning you used\n",
    "workers = 4          # adjust to your CPU cores\n",
    "sg = 1               # 1 = Skip-Gram, 0 = CBOW\n",
    "negative = 10        # number of negative samples\n",
    "epochs = 5\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=train_sentences,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers,\n",
    "    sg=sg,\n",
    "    negative=negative,\n",
    "    epochs=epochs,\n",
    ")\n",
    "\n",
    "# Access the keyed vectors\n",
    "kv = w2v.wv\n",
    "print(\"Vocab size learned:\", len(kv))\n",
    "print(\"Vector for 'language' (if present):\", kv[\"language\"][:10] if \"language\" in kv else \"not in vocab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d539e",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Quick Sanity Checks: Most Similar Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_words = [\"language\", \"research\", \"city\", \"music\", \"war\"]\n",
    "for q in query_words:\n",
    "    if q in kv:\n",
    "        print(f\"\\nMost similar to '{q}':\")\n",
    "        for w, score in kv.most_similar(q, topn=10):\n",
    "            print(f\"  {w:15s}  {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n'{q}' not in vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c696f",
   "metadata": {},
   "source": [
    "\n",
    "## 7. (Optional) Visualize Embeddings (PCA)\n",
    "\n",
    "We'll pick a small list of words and project their vectors to 2D using PCA for a quick plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_words(words):\n",
    "    present = [w for w in words if w in kv]\n",
    "    if not present:\n",
    "        print(\"None of the chosen words are in the vocabulary yet.\")\n",
    "        return\n",
    "    X = np.vstack([kv[w] for w in present])\n",
    "\n",
    "    # PCA to 2D\n",
    "    X = X - X.mean(axis=0, keepdims=True)\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    X2d = X @ Vt[:2].T\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(X2d[:, 0], X2d[:, 1])\n",
    "    for i, w in enumerate(present):\n",
    "        plt.annotate(w, (X2d[i, 0], X2d[i, 1]))\n",
    "    plt.title(\"Word2Vec Embeddings (2D PCA)\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()\n",
    "\n",
    "plot_words([\"king\", \"queen\", \"man\", \"woman\", \"london\", \"paris\", \"music\", \"art\", \"science\", \"research\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379259d3",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Notes: Manual Skipâ€‘Grams vs Gensim Training\n",
    "\n",
    "- The list `pairs_demo` we created earlier is **for understanding/inspection**.  \n",
    "- Gensimâ€™s `Word2Vec` expects raw **sentences of tokens** and handles windowing, negative sampling, and training internally when `sg=1`.  \n",
    "- If you want to implement Skipâ€‘Gram **from scratch**, you can use the `pairs_demo` to drive a small PyTorch training loop with negative sampling â€” but for practical usage, Gensim is optimized and much faster.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
